{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d057e6b1-281d-41c0-9bd4-c6eab79798e6",
   "metadata": {},
   "source": [
    "<center><img src=\"img/torch.png\" alt=\"drawing\" width=\"300\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3636ed06-133f-4236-82f7-ff31b2b41bdd",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Recursive Neural Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "faf16854-8970-433d-9393-c00da873abe8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import requests\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('fivethirtyeight')\n",
    "plt.rcParams.update({'figure.figsize':(6,3), 'legend.loc':\"best\", 'lines.linewidth':1.5, 'lines.marker':\".\", \"image.cmap\":\"tab10\", 'axes.prop_cycle':plt.cycler(color=plt.cm.tab10.colors), 'axes.formatter.useoffset':False, 'axes.titlesize': 12,'axes.labelsize': 10,'ytick.labelsize':8,'xtick.labelsize':8,'legend.fontsize': 10})\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "from torch import nn\n",
    "import torchmetrics\n",
    "from torchinfo import summary\n",
    "import mlflow\n",
    "from helper_functions import train_val_loss_plot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df273eea-0d3c-444c-9076-4bfa5cbfdfff",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Twitter Sentiment Analysis\n",
    "\n",
    "In the first project we will be using the *Real or Not?* dataset from Kaggle which contains text-based Tweets about natural disasters. Out task is to train a sentiment classifier able to figure out if a message is about a disaster or not."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94fc004d-ef0b-44f5-9e28-b10699d94ece",
   "metadata": {},
   "source": [
    "### Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4b09c2ac-28e9-44fa-ab6f-530f2213c901",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Complete Dataset Shape: (7613, 5)\n",
      "- Complete Dataset Targets: {0: 4342, 1: 3271}\n",
      "-------------------------\n",
      "- Train/Test Shapes: X_train: (6851,) | X_test: (762,) | y_train: (6851,) | y_test: (762,)\n",
      "- Train/Test Targets: y_train: {0: 3907, 1: 2944} | y_test: {0: 435, 1: 327}\n"
     ]
    }
   ],
   "source": [
    "def get_data():\n",
    "    # load dataframe\n",
    "    df = pd.read_csv(\"data/twitter.csv\").sample(frac=1, random_state=42)\n",
    "    \n",
    "    print(f\"- Complete Dataset Shape: {df.shape}\")\n",
    "    print(f\"- Complete Dataset Targets: {str(df.target.value_counts().to_dict())}\")\n",
    "    print(25*\"-\")\n",
    "    \n",
    "    # split train and test dataset\n",
    "    X,y = df.text, df.target\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=42, stratify=y)\n",
    "    \n",
    "    print(f\"- Train/Test Shapes: X_train: {X_train.shape} | X_test: {X_test.shape} | y_train: {y_train.shape} | y_test: {y_test.shape}\")\n",
    "    print(f\"- Train/Test Targets: y_train: {str(y_train.value_counts().to_dict())} | y_test: {str(y_test.value_counts().to_dict())}\")\n",
    "    \n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "X_train, X_test, y_train, y_test = get_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b3e610d-7938-44cb-8b22-d54084665431",
   "metadata": {
    "tags": []
   },
   "source": [
    "> **Tokenization**: A direct mapping from either word (every word in a sequence considered a single token), or character (every word in a sequence considered a single token), or sub-word (smaller parts of invidual words into tokens), to a numerical value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "db86d232-ee61-4e54-8e15-86bbb1707fd4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique characters: 122\n",
      "-------------------------\n",
      "- Random Sample: @Allahsfinest12 ...death to muslims -> 1\n",
      "- Random Sample Encoded: [ 27  55  28  28  54 120  75  29  92  68 118  75 100  57  48  35 106 106\n",
      " 106  86 118  54 100 120  35 100  36  35  34  13  75  28  92  34  75   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0] -> 1\n"
     ]
    }
   ],
   "source": [
    "def tokenize_data(X_train, X_test, token):\n",
    "    tweets_list = []\n",
    "    X_train.apply(lambda x: tweets_list.append(x))\n",
    "    X_test.apply(lambda x: tweets_list.append(x))\n",
    "    \n",
    "    if token == \"word\":\n",
    "        unique_tokens = set([word for sentence in tweets_list for word in sentence.split(\" \")])\n",
    "    elif token == \"character\":\n",
    "        unique_tokens = set([character for sentence in tweets_list for character in sentence])\n",
    "    else:\n",
    "        return (\"Error: token must be either word or character\")\n",
    "    \n",
    "    token_to_int_dict = {token:num+1 for num,token in enumerate(unique_tokens)}\n",
    "    \n",
    "    print(f\"Number of unique {token}s: {len(token_to_int_dict)}\")\n",
    "    print(25*\"-\")\n",
    "    \n",
    "    if token == \"word\":\n",
    "        X_train_tokenized = X_train.apply(lambda x: np.array([token_to_int_dict[token] for token in x.split(\" \")]))\n",
    "        X_test_tokenized = X_test.apply(lambda x: np.array([token_to_int_dict[token] for token in x.split(\" \")]))\n",
    "    elif token == \"character\":\n",
    "        X_train_tokenized = X_train.apply(lambda x: np.array([token_to_int_dict[token] for token in x]))\n",
    "        X_test_tokenized = X_test.apply(lambda x: np.array([token_to_int_dict[token] for token in x]))\n",
    "        \n",
    "    # fill in missing dimensions\n",
    "    max_ndim = max(X_train_tokenized.apply(lambda x: len(x)).max(), X_train_tokenized.apply(lambda x: len(x)).max())\n",
    "    \n",
    "    X_train_tokenized = X_train_tokenized.apply(lambda x: np.pad(x, (0,max_ndim - x.shape[0]), mode='constant'))\n",
    "    X_test_tokenized = X_test_tokenized.apply(lambda x: np.pad(x, (0,max_ndim - x.shape[0]), mode='constant'))\n",
    "\n",
    "    # print a random sample\n",
    "    random_int = np.random.randint(X_train.shape[0])\n",
    "    print(f\"- Random Sample: {X_train.reset_index().text[random_int]} -> {y_train.reset_index().target[random_int]}\")\n",
    "    print(f\"- Random Sample Encoded: {X_train_tokenized.reset_index().text[random_int]} -> {y_train.reset_index().target[random_int]}\")\n",
    "    \n",
    "    return X_train_tokenized, X_test_tokenized, token_to_int_dict\n",
    "\n",
    "X_train_tokenized, X_test_tokenized, token_to_int_dict = tokenize_data(X_train, X_test, token=\"character\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01cbcd7b-7f08-46e9-a428-788f9fb72f8b",
   "metadata": {
    "tags": []
   },
   "source": [
    "> **Embedding**: A representation of natural language in the form of a feature vector which can be learned. One can either create her own embedding or reuse a prelearned embedding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d6a6e98-6663-4046-963b-c59e5fe12ee5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def embed_data(X_train_tokenized, X_test_tokenized, token_to_int_dict):\n",
    "    num_embeddings = len(token_to_int_dict)\n",
    "    torch.nn.Embedding(num_embeddings=num_embeddings, embedding_dim=15)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dd607bc-a717-42c2-9a6b-fa810adf44cf",
   "metadata": {},
   "source": [
    "### Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f76f7c80-aa92-45e9-b269-f15f187cdc07",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Classifier(nn.Module):\n",
    "    def __init__(self, in_features: int, out_features: int):\n",
    "        super().__init__()\n",
    "       \n",
    "        self.input_layer = nn.Linear(in_features=in_features, out_features=1)\n",
    "        self.output_layer = nn.Sigmoid()\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return self.output_layer(self.input_layer(x))\n",
    "    \n",
    "    def model_summary(self, input_size):\n",
    "        return summary(self, input_size=input_size, col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\"], col_width=15, row_settings=[\"var_names\"])\n",
    "\n",
    "classifier = Classifier(in_features=2, out_features=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75833480-0dc4-4d32-8b8c-afa2a791a057",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(\n",
    "    experiment_name: str,\n",
    "    model: nn.Module,\n",
    "    data: (torch.utils.data.DataLoader, torch.utils.data.DataLoader),\n",
    "    loss_fn: nn.Module,\n",
    "    optimizer: torch.optim.Optimizer,\n",
    "    metrics: list,\n",
    "    epochs: int,\n",
    "    description: str = None\n",
    "):\n",
    "    mlflow.set_experiment(experiment_name=experiment_name)    \n",
    "    \n",
    "    with mlflow.start_run(\n",
    "        run_name=f\"{model.__class__.__name__} {datetime.now().strftime('%Y-%m-%d-%H:%M:%S')}\",\n",
    "        description=description):\n",
    " \n",
    "        train_dataloader, test_dataloader = data\n",
    "        \n",
    "        for epoch in range(epochs):   \n",
    "            model.train()\n",
    "            \n",
    "            train_loss = 0\n",
    "            validation_loss = 0\n",
    "            metrics_dict = {}   \n",
    "            for metric in metrics:\n",
    "                metrics_dict[f\"train_{metric.__class__.__name__}\"] = 0\n",
    "                metrics_dict[f\"validation_{metric.__class__.__name__}\"] = 0\n",
    "\n",
    "            # train in batches\n",
    "            for batch, (X_train, y_train) in enumerate(train_dataloader):\n",
    "                y_logits = model(X_train)\n",
    "                loss = loss_fn(y_logits, y_train)\n",
    "                train_loss += loss.item() / len(train_dataloader) # divide by length of train_dataloader to get average per batch\n",
    "                y_pred = torch.argmax(torch.softmax(y_logits, dim=1), dim=1)\n",
    "                \n",
    "                for metric in metrics:\n",
    "                    # divide by length of train_dataloader to get average per batch\n",
    "                    metrics_dict[f\"train_{metric.__class__.__name__}\"] += metric(y_pred, y_train).item() / len(train_dataloader)\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "            \n",
    "            model.eval()\n",
    "            with torch.inference_mode():\n",
    "                for batch, (X_test, y_test) in enumerate(test_dataloader): \n",
    "                    y_logits = model(X_test)\n",
    "                    loss = loss_fn(y_logits, y_test)\n",
    "                    validation_loss += loss.item() / len(test_dataloader) # divide by length of test_dataloader to get average per batch\n",
    "                    y_pred = torch.argmax(torch.softmax(y_logits, dim=1), dim=1)\n",
    "                    \n",
    "                    for metric in metrics:\n",
    "                        # divide by length of test_dataloader to get average per batch\n",
    "                        metrics_dict[f\"validation_{metric.__class__.__name__}\"] += metric(y_pred, y_test).item() / len(test_dataloader)\n",
    "\n",
    "            if epoch % (epochs/10) == 0:\n",
    "                print(f\"Epoch: {epoch} | Train Loss: {train_loss:.3f} | Validation Loss: {validation_loss:.3f}\")\n",
    "            \n",
    "            mlflow.log_metrics({\n",
    "                \"train_loss\": train_loss,\n",
    "                \"validation_loss\": validation_loss\n",
    "            }, step=epoch)\n",
    "            \n",
    "            for metric_name, metric_value in metrics_dict.items():\n",
    "                mlflow.log_metric(key=metric_name, value=metric_value, step=epoch)\n",
    "\n",
    "        mlflow.pytorch.log_model(model, \"model\")\n",
    "        mlflow.log_params({\n",
    "            \"epochs\": epochs,\n",
    "            \"optimizer\": optimizer.__class__.__name__,\n",
    "            \"lr\": optimizer.param_groups[0][\"lr\"]\n",
    "        })\n",
    "        fig = train_val_loss_plot(run_id= mlflow.active_run().info.run_id, plot=True)\n",
    "        mlflow.log_figure(fig, \"plots/train_validation_loss_curves.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dd46cd1-ba0f-46c6-950c-0a0b385b5ca7",
   "metadata": {},
   "source": [
    "## Generate New Text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1f7312b-b988-441f-ad88-11ac8d939dcf",
   "metadata": {},
   "source": [
    "### Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8079c82-a16b-4173-b967-11d1d0430ed6",
   "metadata": {},
   "source": [
    "In this notebook we will develop a model that accepts as input a text document, and can generate new text that is similar in style to the input document. More specificaly, we will use the book \"The Mysterious Island\", by Jules Verne in plain text format.\n",
    "\n",
    "In character-level language modeling, the input is broken down into a sequence of characters that are fed into our network one character at a time. The network will process each new character in conjunction with the memory of the previously seen characters to predict the next one.\n",
    "\n",
    "<center><img src=\"img/torch_04_01.png\" alt=\"drawing\" width=\"500\"/></center>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99849dad-cac9-4a59-a7b1-780db9579216",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def download_text():\n",
    "    text = requests.get(\"https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\").text\n",
    "    print(f\"Dataset Downloaded\")  \n",
    "    \n",
    "    vocabulary = sorted(set(text))\n",
    "    print(f\"- Length of dataset in characters: {len(text)} \\n- Unique number of characters: {len(vocabulary)}\")  \n",
    "    print(f\"- Unique characters:{''.join(vocabulary[1:])}\")\n",
    "    print(f\"- Sample:\\n\\n{text[:1000]}\")\n",
    "\n",
    "    return text\n",
    "    \n",
    "text = download_text()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f38da6d-73f5-4352-8a25-664b87488658",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def make_dataloader(text_dir, seq_length, batch_size):\n",
    "    \n",
    "    # unique characters\n",
    "    char_set = sorted(set(text))\n",
    "    \n",
    "    # convert text into a numeric format\n",
    "    char_to_int_dict = {c:i for i,c in enumerate(char_set)}\n",
    "    text_encoded = [char_to_int_dict[c] for c in text]\n",
    "    assert len(text) == len(text_encoded)\n",
    "    \n",
    "    # create chunks\n",
    "    text_chunks = [\n",
    "        text_encoded[i:(i + seq_length) + 1] for i in range(len(text_encoded) - (seq_length+1))\n",
    "    ]\n",
    "    \n",
    "    # create dataset object\n",
    "    X,y = torch.Tensor(text_chunks[:-1]).long(), torch.Tensor(text_chunks[1:]).long()\n",
    "    dataset = torch.utils.data.dataset.TensorDataset(X, y)\n",
    "    \n",
    "    # create dataloader object\n",
    "    dataloader = DataLoader(dataset=dataset, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "    \n",
    "    # log some information\n",
    "    print(f'Total Text Length: {len(text)} | Unique Characters: {len(char_set)} | Dataloader: {len(dataloader)} Batches Of Size {batch_size}')\n",
    "    \n",
    "    # create decoder for when the model is done\n",
    "    int_to_char = {i:c for c,i in char_to_int_dict.items()}\n",
    "    def decoder(encoding):\n",
    "        return \"\".join([int_to_char[i] for i in encoding.numpy()])\n",
    "    dataloader.decoder = decoder\n",
    "\n",
    "    return dataloader\n",
    "\n",
    "dataloader = make_dataloader(text_dir='data/the_mysterious_island.txt', seq_length=40, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96a5ded4-1179-43f0-a157-87c1e7bfeda4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# a random (decoded) example\n",
    "X,y = dataloader.dataset[np.random.randint(0, len(dataloader.dataset))]\n",
    "print(f'Input: \"{dataloader.decoder(X)}\" -> Target: \"{dataloader.decoder(y)}\"')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74e32518-099f-40bc-8b03-87ed8f8668de",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81197622-cb9a-451e-8475-b39e82b5b082",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class RNN(nn.Module):\n",
    "    \n",
    "    def __init__(self, vocab_size, embed_dim, rnn_hidden_size):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(\n",
    "            num_embeddings=vocab_size, \n",
    "            embedding_dim=embed_dim)\n",
    "        self.rnn_hidden_size = rnn_hidden_size\n",
    "        self.rnn = nn.LSTM(embed_dim, rnn_hidden_size, batch_first=True)\n",
    "        self.fc = nn.Linear(in_features=rnn_hidden_size, out_features=vocab_size)\n",
    "\n",
    "    def forward(self, x, hidden, cell):\n",
    "        out = self.embedding(x).unsqueeze(1)\n",
    "        out, (hidden, cell) = self.rnn(out, (hidden, cell))\n",
    "        out = self.fc(out).reshape(out.size(0), -1)\n",
    "        return out, hidden, cell\n",
    "\n",
    "    def init_hidden(self, batch_size):\n",
    "        hidden = torch.zeros(1, batch_size, self.rnn_hidden_size)\n",
    "        cell = torch.zeros(1, batch_size, self.rnn_hidden_size)\n",
    "        return hidden, cell\n",
    "    \n",
    "model = RNN(vocab_size=80, embed_dim=256, rnn_hidden_size=512)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  },
  "toc-autonumbering": true,
  "toc-showcode": false,
  "toc-showtags": false
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
